{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, y, weights, bias): \n",
    "    \"\"\"\n",
    "    x: training data as a vector (nparray), where each value corresponds\n",
    "        to a feature's value\n",
    "    y: label (0 or 1)\n",
    "    weights: weights of the perceptron\n",
    "    bias: bias\n",
    "    \"\"\"\n",
    "    y_pred = predict(x, weights, bias)\n",
    "    loss = (y_pred - y)**2   \n",
    "    d_loss = 2*(y_pred - y)\n",
    "    \n",
    "    return y_pred, loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_perceptron(x, y, learning_rate):\n",
    "    \"\"\"\n",
    "    Optimizes the Perceptron's weights by looping over the same steps for as many epochs as the user wants.\n",
    "    Steps:\n",
    "    1. Forward propagate data point\n",
    "    2. Backpropagate\n",
    "    3. Update weights\n",
    "    4. Check stop conditions while looping\n",
    "    \n",
    "    It is worth noting that a history of the Perceptron's losses over each epoch is kept,\n",
    "    which will be used\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    error = 999\n",
    "    weights = np.random.rand(x.shape[1])\n",
    "    bias = np.random.rand()\n",
    "    \n",
    "    errors = list()\n",
    "    epochs = list()\n",
    "    \n",
    "    # Loop until stop conditions are met\n",
    "    while (epoch <= 1000) and (error > 9e-4):\n",
    "        \n",
    "        loss_ = 0\n",
    "        # Loop over every data point\n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            # Forward Propagation on each data point\n",
    "            y_pred, loss, d_loss = forward_propagation(x[i], y[i], weights, bias)\n",
    "\n",
    "            # Backpropagation\n",
    "            partial_derivates = backpropagation(x[i], d_loss)\n",
    "            \n",
    "            # Learn by updating the weights of the perceptron\n",
    "            weights = weights - (learning_rate * np.array(partial_derivates))\n",
    "\n",
    "        # Evaluate the results\n",
    "        for index, feature_value_test in enumerate(x):\n",
    "            y_pred, loss, d_loss = forward_propagation(feature_value_test, y[index], weights, bias)\n",
    "            loss_ += loss\n",
    "\n",
    "        errors.append(loss_/len(x))\n",
    "        epochs.append(epoch)\n",
    "        error = errors[-1]\n",
    "        epoch += 1\n",
    "\n",
    "        print('Epoch {}. loss: {}'.format(epoch, errors[-1]))\n",
    "\n",
    "    \n",
    "    return weights, bias, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(prediction):\n",
    "    \"\"\"\n",
    "    Receives the output of the perceptron's function as parameter, and applies the\n",
    "    activation function on it.\n",
    "    For the purpose of this project, the activation function maps the negative outputs\n",
    "    to 0 and the positive ones to 1\n",
    "    \"\"\"\n",
    "    if prediction >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "  \n",
    "\n",
    "def predict(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Predicts the class of a given data point (x), by applying the Perceptron's \n",
    "    function, and the activation function lastly.\n",
    "    As both weights and x are vectors, the dot product is used.\n",
    "    \"\"\"\n",
    "    prediction = np.dot(weights, x) + bias\n",
    "    prediction = activation_function(prediction)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(x, d_loss):\n",
    "    \"\"\"\n",
    "    Performs the Backpropagation step on a given data point.\n",
    "    Receives as input the data point, the Perceptron's weights and the partial derivative of the loss\n",
    "    over the predicted y.\n",
    "    The received derivative is used to calculate the partial derivative of the loss over the weight of each feature.\n",
    "    A list with the partial derivatives of the loss over each weight is returned.\n",
    "    \"\"\"\n",
    "    partial_derivates = list()\n",
    "    for feature_value in x:\n",
    "        partial_derivates.append(d_loss*feature_value)\n",
    "        \n",
    "    return partial_derivates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8816, 785)\n",
      "[0 1]\n",
      "(6612, 784)\n",
      "(2204, 784)\n",
      "(6612,)\n",
      "(2204,)\n",
      "Values before rescaling:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255]\n",
      "Values after rescaling:  [0.         0.00392157 0.00784314 0.01176471 0.01568627 0.01960784\n",
      " 0.02352941 0.02745098 0.03137255 0.03529412 0.03921569 0.04313725\n",
      " 0.04705882 0.05098039 0.05490196 0.05882353 0.0627451  0.06666667\n",
      " 0.07058824 0.0745098  0.07843137 0.08235294 0.08627451 0.09019608\n",
      " 0.09411765 0.09803922 0.10196078 0.10588235 0.10980392 0.11372549\n",
      " 0.11764706 0.12156863 0.1254902  0.12941176 0.13333333 0.1372549\n",
      " 0.14117647 0.14509804 0.14901961 0.15294118 0.15686275 0.16078431\n",
      " 0.16470588 0.16862745 0.17254902 0.17647059 0.18039216 0.18431373\n",
      " 0.18823529 0.19215686 0.19607843 0.2        0.20392157 0.20784314\n",
      " 0.21176471 0.21568627 0.21960784 0.22352941 0.22745098 0.23137255\n",
      " 0.23529412 0.23921569 0.24313725 0.24705882 0.25098039 0.25490196\n",
      " 0.25882353 0.2627451  0.26666667 0.27058824 0.2745098  0.27843137\n",
      " 0.28235294 0.28627451 0.29019608 0.29411765 0.29803922 0.30196078\n",
      " 0.30588235 0.30980392 0.31372549 0.31764706 0.32156863 0.3254902\n",
      " 0.32941176 0.33333333 0.3372549  0.34117647 0.34509804 0.34901961\n",
      " 0.35294118 0.35686275 0.36078431 0.36470588 0.36862745 0.37254902\n",
      " 0.37647059 0.38039216 0.38431373 0.38823529 0.39215686 0.39607843\n",
      " 0.4        0.40392157 0.40784314 0.41176471 0.41568627 0.41960784\n",
      " 0.42352941 0.42745098 0.43137255 0.43529412 0.43921569 0.44313725\n",
      " 0.44705882 0.45098039 0.45490196 0.45882353 0.4627451  0.46666667\n",
      " 0.47058824 0.4745098  0.47843137 0.48235294 0.48627451 0.49019608\n",
      " 0.49411765 0.49803922 0.50196078 0.50588235 0.50980392 0.51372549\n",
      " 0.51764706 0.52156863 0.5254902  0.52941176 0.53333333 0.5372549\n",
      " 0.54117647 0.54509804 0.54901961 0.55294118 0.55686275 0.56078431\n",
      " 0.56470588 0.56862745 0.57254902 0.57647059 0.58039216 0.58431373\n",
      " 0.58823529 0.59215686 0.59607843 0.6        0.60392157 0.60784314\n",
      " 0.61176471 0.61568627 0.61960784 0.62352941 0.62745098 0.63137255\n",
      " 0.63529412 0.63921569 0.64313725 0.64705882 0.65098039 0.65490196\n",
      " 0.65882353 0.6627451  0.66666667 0.67058824 0.6745098  0.67843137\n",
      " 0.68235294 0.68627451 0.69019608 0.69411765 0.69803922 0.70196078\n",
      " 0.70588235 0.70980392 0.71372549 0.71764706 0.72156863 0.7254902\n",
      " 0.72941176 0.73333333 0.7372549  0.74117647 0.74509804 0.74901961\n",
      " 0.75294118 0.75686275 0.76078431 0.76470588 0.76862745 0.77254902\n",
      " 0.77647059 0.78039216 0.78431373 0.78823529 0.79215686 0.79607843\n",
      " 0.8        0.80392157 0.80784314 0.81176471 0.81568627 0.81960784\n",
      " 0.82352941 0.82745098 0.83137255 0.83529412 0.83921569 0.84313725\n",
      " 0.84705882 0.85098039 0.85490196 0.85882353 0.8627451  0.86666667\n",
      " 0.87058824 0.8745098  0.87843137 0.88235294 0.88627451 0.89019608\n",
      " 0.89411765 0.89803922 0.90196078 0.90588235 0.90980392 0.91372549\n",
      " 0.91764706 0.92156863 0.9254902  0.92941176 0.93333333 0.9372549\n",
      " 0.94117647 0.94509804 0.94901961 0.95294118 0.95686275 0.96078431\n",
      " 0.96470588 0.96862745 0.97254902 0.97647059 0.98039216 0.98431373\n",
      " 0.98823529 0.99215686 0.99607843 1.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('data_mnist.csv')\n",
    "\n",
    "# Take only data with labels 1\n",
    "data_ones = data[data['label'] == 1]\n",
    "\n",
    "# Take only data with labels 0\n",
    "data_zeros = data[data['label'] == 0]\n",
    "\n",
    "# Concatenate instances with label 0 and 1\n",
    "data = pd.concat([data_ones, data_zeros])\n",
    "print(data.shape)\n",
    "print(np.unique(data['label'].to_numpy()))\n",
    "\n",
    "\n",
    "# Split dataset with 75% training data and 25% test data\n",
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=1, shuffle=True)\n",
    "\n",
    "# Split datasets into features and labels\n",
    "x_train = train_data.drop('label', axis=1).to_numpy()\n",
    "x_test = test_data.drop('label', axis=1).to_numpy()\n",
    "y_train = train_data['label'].to_numpy()\n",
    "y_test = test_data['label'].to_numpy()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print('Values before rescaling: ', np.unique(x_train))\n",
    "\n",
    "# Rescale data points to values between 0 and 1 (pixels are originally 0-255)\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "print('Values after rescaling: ', np.unique(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x_test, y_test, weights, bias):\n",
    "    \n",
    "    # Initialize True Positive, True Negative, False Positive and False Negative\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "    for sample, label in zip(x_test, y_test):\n",
    "\n",
    "        prediction = predict(sample, weights, bias)\n",
    "\n",
    "        if prediction == label:\n",
    "            if prediction == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 0.012401693889897156\n",
      "Epoch 2. loss: 0.008318209316394435\n",
      "Epoch 3. loss: 0.006503327283726558\n",
      "Epoch 4. loss: 0.0054446460980036296\n",
      "Epoch 5. loss: 0.004688445251058681\n",
      "Epoch 6. loss: 0.0043859649122807015\n",
      "Epoch 7. loss: 0.003932244404113733\n",
      "Epoch 8. loss: 0.003176043557168784\n",
      "Epoch 9. loss: 0.0024198427102238356\n",
      "Epoch 10. loss: 0.002268602540834846\n",
      "Epoch 11. loss: 0.0024198427102238356\n",
      "Epoch 12. loss: 0.002117362371445856\n",
      "Epoch 13. loss: 0.0019661222020568663\n",
      "Epoch 14. loss: 0.002117362371445856\n",
      "Epoch 15. loss: 0.0018148820326678765\n",
      "Epoch 16. loss: 0.0019661222020568663\n",
      "Epoch 17. loss: 0.0019661222020568663\n",
      "Epoch 18. loss: 0.0018148820326678765\n",
      "Epoch 19. loss: 0.0019661222020568663\n",
      "Epoch 20. loss: 0.001663641863278887\n",
      "Epoch 21. loss: 0.002117362371445856\n",
      "Epoch 22. loss: 0.002117362371445856\n",
      "Epoch 23. loss: 0.0018148820326678765\n",
      "Epoch 24. loss: 0.0015124016938898972\n",
      "Epoch 25. loss: 0.0018148820326678765\n",
      "Epoch 26. loss: 0.001058681185722928\n",
      "Epoch 27. loss: 0.001663641863278887\n",
      "Epoch 28. loss: 0.001058681185722928\n",
      "Epoch 29. loss: 0.0015124016938898972\n",
      "Epoch 30. loss: 0.001058681185722928\n",
      "Epoch 31. loss: 0.0012099213551119178\n",
      "Epoch 32. loss: 0.0009074410163339383\n",
      "Epoch 33. loss: 0.0013611615245009074\n",
      "Epoch 34. loss: 0.001058681185722928\n",
      "Epoch 35. loss: 0.001058681185722928\n",
      "Epoch 36. loss: 0.0009074410163339383\n",
      "Epoch 37. loss: 0.0007562008469449486\n"
     ]
    }
   ],
   "source": [
    "weights, bias, errors = optimize_perceptron(x_train, y_train, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9972776769509982\n"
     ]
    }
   ],
   "source": [
    "acc = calculate_accuracy(x_test, y_test, weights, bias)\n",
    "print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights.resize((28, 28))\n",
    "plt.imshow(weights)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
